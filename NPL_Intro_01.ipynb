{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NATURAL LANGUAGE PROCESSING\n",
        "\n",
        "## Week 01 - Intro NLP\n",
        "## Python"
      ],
      "metadata": {
        "id": "c8kfdXkuN5fM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK"
      ],
      "metadata": {
        "id": "NKbkehyFQzYR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36XhkXlVN4q1"
      },
      "outputs": [],
      "source": [
        "#@title Library\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Corpora"
      ],
      "metadata": {
        "id": "E2KUPpmUOHbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Corpus (text)\n",
        "nltk.download(info_or_id='machado', quiet=True)\n",
        "# Sentence tokenizing Library\n",
        "nltk.download(info_or_id='punkt', quiet=True)\n",
        "# Corpus Import\n",
        "from nltk.corpus import machado"
      ],
      "metadata": {
        "id": "Pm6wTXw6OD9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Corpus Doc´s\n",
        "machado.fileids()[140:145]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_yG3BPrOsz0",
        "outputId": "c7d140be-ffdd-4aa5-87fb-5b422c962efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['critica/mact04.txt',\n",
              " 'critica/mact05.txt',\n",
              " 'critica/mact06.txt',\n",
              " 'critica/mact07.txt',\n",
              " 'critica/mact08.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Qtty Corpus Doc´s\n",
        "len(machado.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PiCweYRPmpj",
        "outputId": "74933232-3324-4736-8f7e-8e728c32e5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "246"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Doc Content critica/mact05.txt\n",
        "print(machado.raw('critica/mact05.txt')[:598])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g3fDVB1P7Se",
        "outputId": "0d799e74-ff45-4ba8-ac8e-ce4cda503a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crítica, A crítica teatral, A crítica teatral. José de Alencar : Mãe,\n",
            "1860\n",
            "\n",
            "A crítica\n",
            "teatral. José de Alencar: Mãe\n",
            "\n",
            "Texto-Fonte:\n",
            "\n",
            "Obra Completa de Machado de Assis,\n",
            "\n",
            "Rio\n",
            "de Janeiro: Nova Aguilar, vol. III, 1994.\n",
            "\n",
            "Publicado\n",
            "originalmente na Revista Dramática, seção do Diário do Rio de Janeiro,\n",
            "29/03/1860.\n",
            "\n",
            "Escrever\n",
            "crítica e crítica de teatro não é só uma tarefa difícil, é também uma empresa\n",
            "arriscada.\n",
            "\n",
            "A razão é\n",
            "simples. No dia em que a pena, fiel ao preceito da censura, toca um ponto negro\n",
            "e olvida por momentos a estrofe laudatória, as inimizades levantam-se de\n",
            "envolta com as calúnias.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopwords"
      ],
      "metadata": {
        "id": "eM9-ICtvRltv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Stopwords\n",
        "nltk.download(info_or_id='stopwords', quiet=True)\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')"
      ],
      "metadata": {
        "id": "U4e21oBgRht2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exibit 10 words\n",
        "print(stopwords[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCgZBFYoRk0D",
        "outputId": "9692171a-9584-414f-f484-bb559a92c318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization"
      ],
      "metadata": {
        "id": "QDpqYOH6TIIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download - tokenizers (sentences and words)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "uISEgld_SK-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = machado.raw('contos/macn001.txt')"
      ],
      "metadata": {
        "id": "w6R02y1lUupl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Text split by sentence\n",
        "sentencas = sent_tokenize(text=texto, language='portuguese')\n",
        "sentencas[50:53]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FMqUhw6VHPD",
        "outputId": "6bfcf67a-21b1-4b5e-d7d9-dc860acd91f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A cadelinha entrou a acompanhá-lo, e\\nele, notando que era animal sem dono visível, levou-a consigo para os\\nCajueiros.',\n",
              " 'Apenas entrou em casa examinou\\ncuidadosamente a cadelinha, Miss Dollar era realmente um mimo; tinha as\\nformas delgadas e graciosas da sua fidalga raça; os olhos castanhos e\\naveludados pareciam exprimir a mais completa felicidade deste mundo, tão\\nalegres e serenos eram.',\n",
              " 'Mendonça contemplou-a e examinou minuciosamente.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Text split by words\n",
        "palavras = word_tokenize(text=texto, language='portuguese')\n",
        "print(palavras[100:110])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at_-XMWtV7Mi",
        "outputId": "7b50b545-e7be-4728-cf93-7729472c7646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['saber', 'quem', 'era', 'Miss', 'Dollar', '.', 'Mas', 'por', 'outro', 'lado']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Twitter Tokens\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "sample_twitter = \"I'm very veryyyyy happyyyyy today!! #beterlife @barneys :P :D\"\n",
        "tt1 = TweetTokenizer()\n",
        "tt2 = TweetTokenizer(strip_handles=True, reduce_len=True)"
      ],
      "metadata": {
        "id": "UgDNtJoYW_xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@ Split methods comaparation\n",
        "\n",
        "print('word_tokenize')\n",
        "print(word_tokenize(text=sample_twitter))\n",
        "print('TweetTokenizer')\n",
        "print(tt1.tokenize(sample_twitter))\n",
        "print('TweetTokenizer Reduce')\n",
        "print(tt2.tokenize(sample_twitter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKH7UimYY20i",
        "outputId": "aa1fe7b0-bdd5-4283-ddf5-4024c062ca40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_tokenize\n",
            "['I', \"'m\", 'very', 'veryyyyy', 'happyyyyy', 'today', '!', '!', '#', 'beterlife', '@', 'barneys', ':', 'P', ':', 'D']\n",
            "TweetTokenizer\n",
            "[\"I'm\", 'very', 'veryyyyy', 'happyyyyy', 'today', '!', '!', '#beterlife', '@barneys', ':P', ':D']\n",
            "TweetTokenizer Reduce\n",
            "[\"I'm\", 'very', 'veryyy', 'happyyy', 'today', '!', '!', '#beterlife', ':P', ':D']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Words Radical´s"
      ],
      "metadata": {
        "id": "pt-R8Vt1u2FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title List of languages\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "SnowballStemmer.languages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZP4cRZHujQq",
        "outputId": "a85f4bf4-fdf6-4bb1-eb90-60a9df5d3305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Portuguese\n",
        "stemmer = SnowballStemmer(language='portuguese')"
      ],
      "metadata": {
        "id": "nwoWkZW3ujN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example computador, computação, computando\n",
        "palavras = ['computador', 'computação', 'computando', 'computado', 'computar']\n",
        "for p in palavras:\n",
        "  print(f'{p:10} => {stemmer.stem(p)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3Y-UWhLv41Q",
        "outputId": "21c0ebd0-a4b9-4c4f-e6ce-b93ce6b99959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computador => comput\n",
            "computação => comput\n",
            "computando => comput\n",
            "computado  => comput\n",
            "computar   => comput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lematização, simplificação de palavra"
      ],
      "metadata": {
        "id": "IJaUiUu5xd6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title donwload e importação do objeto\n",
        "nltk.download('wordnet', quiet=True)\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "86R79Y-gxnS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Criação do objeto para lematização\n",
        "wordNetLemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "bByA2eOfx-m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lematizando palavras (só em inglês)\n",
        "words = ['works', 'rocks', 'corpora']\n",
        "for w in words:\n",
        "  print(f'{w:10} => {wordNetLemmatizer.lemmatize(w)}')"
      ],
      "metadata": {
        "id": "7-vVG8y3ysUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b490ea-96c6-4f45-dbf1-3a90a6690d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "works      => work\n",
            "rocks      => rock\n",
            "corpora    => corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lematizando palavras (uso sintático: adjetivos)\n",
        "words = ['better', 'worse', 'cheaper', 'expensivest']\n",
        "for w in words:\n",
        "  print(f'{w:11} => {wordNetLemmatizer.lemmatize(w, pos=\"a\")}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10AzSWqN0uDx",
        "outputId": "4930a46d-c327-4dca-f379-7abb1ad59851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "better      => good\n",
            "worse       => bad\n",
            "cheaper     => cheap\n",
            "expensivest => expensive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lematizando palavras (uso sintático: verbos)\n",
        "words = ['clustering', 'broken', 'understood', 'awoken']\n",
        "for w in words:\n",
        "  print(f'{w:11} => {wordNetLemmatizer.lemmatize(w, pos=\"v\")}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plie0EEM3Vpq",
        "outputId": "da7902d1-476a-4ecd-bc3f-313126c14924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clustering  => cluster\n",
            "broken      => break\n",
            "understood  => understand\n",
            "awoken      => awake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS Tagging\n",
        "\n",
        "Classe gramatical dos tokens"
      ],
      "metadata": {
        "id": "91vT8ZUJ38pB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download dos recursos\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('maxent_ne_chunker', quiet=True)\n",
        "nltk.download('words', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAFFesN936cx",
        "outputId": "86c14065-3bd9-4f5e-b356-4d9b206b8ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Obtendo a classe gramatical (em inglês)\n",
        "phrase = 'Rafael is working at Google in the South American'\n",
        "print(nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(phrase))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFw4Ky7M49I2",
        "outputId": "14f53336-9e00-4fb6-d139-16471467b554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Rafael/NNP)\n",
            "  is/VBZ\n",
            "  working/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Google/NNP)\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (LOCATION South/JJ)\n",
            "  (GPE American/JJ))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "teste_tagger = joblib.load('POS_tagger_brill.pkl')\n",
        "phrase = 'O rato roeu a roupa do rei de Roma'\n",
        "teste_tagger.tag(word_tokenize(phrase))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "-P0TE6HM5ogj",
        "outputId": "87f0e383-4120-4638-b32a-749658d302c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-8060298ea3cf>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mteste_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'POS_tagger_brill.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'O rato roeu a roupa do rei de Roma'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mteste_tagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'POS_tagger_brill.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## spaCy"
      ],
      "metadata": {
        "id": "fuVkZHCh8YMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Biblioteca\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "Tupg9FGZ8pvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = 'Rafael is working at Google in the South American. He works very hard :D'"
      ],
      "metadata": {
        "id": "Uz_q1mnc9CyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tokenização - frases\n",
        "sentence = nlp(phrase)\n",
        "for p in sentence.sents:\n",
        "  print(p.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx_fLU4Y9JWR",
        "outputId": "5951d11c-7d51-4500-8d47-2dcdbc794bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rafael is working at Google in the South American.\n",
            "He works very hard :D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tokenização - palavras\n",
        "sentence = nlp(phrase)\n",
        "for word in sentence:\n",
        "  print(word.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKCrE_jo-NDL",
        "outputId": "b8cb9d6a-95cc-4af1-f9ec-b6e2ef94983d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rafael\n",
            "is\n",
            "working\n",
            "at\n",
            "Google\n",
            "in\n",
            "the\n",
            "South\n",
            "American\n",
            ".\n",
            "He\n",
            "works\n",
            "very\n",
            "hard\n",
            ":D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Stopwords (is_stop retorna True/False)\n",
        "for word in sentence:\n",
        "  print(f'{word.text:10} => {word.is_stop}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viMjypIc-NAe",
        "outputId": "558ce668-d023-403c-f464-ab09a207d7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rafael     => False\n",
            "is         => True\n",
            "working    => False\n",
            "at         => True\n",
            "Google     => False\n",
            "in         => True\n",
            "the        => True\n",
            "South      => False\n",
            "American   => False\n",
            ".          => False\n",
            "He         => True\n",
            "works      => False\n",
            "very       => True\n",
            "hard       => False\n",
            ":D         => False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tipo do token (alfabético? True/False)\n",
        "for word in sentence:\n",
        "  print(f'{word.text:10} => {word.is_alpha}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhK0Vr42_Uwt",
        "outputId": "be7157f1-e65f-4e17-a6d0-9e66bf8fce68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rafael     => True\n",
            "is         => True\n",
            "working    => True\n",
            "at         => True\n",
            "Google     => True\n",
            "in         => True\n",
            "the        => True\n",
            "South      => True\n",
            "American   => True\n",
            ".          => False\n",
            "He         => True\n",
            "works      => True\n",
            "very       => True\n",
            "hard       => True\n",
            ":D         => False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Lemma\n",
        "phrase = 'Lucas has drinked two coffees while the computer is computing the values of the matrices.'\n",
        "sentence = nlp(phrase)\n",
        "for word in sentence:\n",
        "  print(f'{word.text:10} => {word.lemma_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52yhg4vD_Uug",
        "outputId": "4f160756-05d4-4c38-a008-0f206615d017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lucas      => Lucas\n",
            "has        => have\n",
            "drinked    => drink\n",
            "two        => two\n",
            "coffees    => coffee\n",
            "while      => while\n",
            "the        => the\n",
            "computer   => computer\n",
            "is         => be\n",
            "computing  => compute\n",
            "the        => the\n",
            "values     => value\n",
            "of         => of\n",
            "the        => the\n",
            "matrices   => matrix\n",
            ".          => .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  POS Tagging (classes gramaticais)\n",
        "for word in sentence:\n",
        "  print(f'{word.text:10} => {word.pos_:6}- {word.tag_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtHoBxkO_Uri",
        "outputId": "4156a92c-7df5-49a7-fac8-256450e4c097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lucas      => PROPN - NNP\n",
            "has        => AUX   - VBZ\n",
            "drinked    => VERB  - VBN\n",
            "two        => NUM   - CD\n",
            "coffees    => NOUN  - NNS\n",
            "while      => SCONJ - IN\n",
            "the        => DET   - DT\n",
            "computer   => NOUN  - NN\n",
            "is         => AUX   - VBZ\n",
            "computing  => VERB  - VBG\n",
            "the        => DET   - DT\n",
            "values     => NOUN  - NNS\n",
            "of         => ADP   - IN\n",
            "the        => DET   - DT\n",
            "matrices   => NOUN  - NNS\n",
            ".          => PUNCT - .\n"
          ]
        }
      ]
    }
  ]
}